{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Discover Cost-Efficient AI Customer Service Agents with NVIDIA Data Flywheel Blueprint\n",
    "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W)\n",
    "\n",
    "In this notebook, you will learn learn how to use the Data Flywheel Foundational Blueprint to continuously discover and promote more cost-efficient agents for an [AI virtual customer service assistant](https://build.nvidia.com/nvidia/ai-virtual-assistant-for-customer-service).\n",
    "\n",
    "### Data Flywheel Blueprint\n",
    "\n",
    "![dfw_arch](../docs/images/data-flywheel-blueprint.png)\n",
    "\n",
    "\n",
    "### AI Virtual Assistant for Customer Service\n",
    "\n",
    "The primary customer service agent in the AI Virtual Assistant uses tool calling to route user queries to specialized assistants, including: \n",
    "\n",
    "- Product Q&A\n",
    "- Order status verification\n",
    "- Returns processing\n",
    "- Small talk and casual engagement\n",
    "\n",
    "These interactions generate logs and tool-calling data that you can use as both evaluation benchmarks and training data. In this tutorial, you'll use this information to drive the flywheel process, fine-tuning smaller LLMs (such as `meta/llama-3.2-1B-instruct`, `meta/llama-3.2-3B-instruct`, `meta/llama-3.1-8B-instruct`) to match accuracy of the currently deployed model (`meta/llama-3.3-70B-instruct`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with the Blueprint\n",
    "\n",
    "The following diagram illustrates how admin tools and applications interact with the Flywheel Blueprint, which orchestrates logging, processing, and model management to enable continuous optimization.\n",
    "\n",
    "![Arch](./arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents \n",
    "\n",
    "0. [Data Flywheel Setup](#0)\n",
    "1. [Load Sample Data](#1)\n",
    "2. [Create a Flywheel Job](#2)\n",
    "3. [Monitor Job Status](#3)\n",
    "4. [Optional: Show Continuous Improvement](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "## Data Flywheel Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup\n",
    "\n",
    "In general, you can start the Data Flywheel service by following the instructions provided in the notebook [README.md](https://github.com/NVIDIA-AI-Blueprints/data-flywheel/blob/main/notebooks/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brev Launchable Setup\n",
    "\n",
    "If youâ€™re running this notebook from a [Brev Launchable](https://brev.nvidia.com/launchable/deploy/now?launchableID=env-2wggjBvDlVp4pLQD8ytZySh5m8W), please complete the following setup steps before proceeding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Set NGC API key following the instructions at [Generating NGC API Keys](https://docs.nvidia.com/ngc/gpu-cloud/ngc-private-registry-user-guide/index.html#generating-api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.environ.get('NGC_API_KEY'):\n",
    "    os.environ['NGC_API_KEY'] = '<your_ngc_api_key>'\n",
    "else:\n",
    "    print(\"NGC_API_KEY already set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.environ.get('WANDB_API_KEY'):\n",
    "    os.environ['WANDB_API_KEY'] = '<your_wandb_api_key>'\n",
    "else:\n",
    "    print(\"WANDB_API_KEY already set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Clone the data flywheel repo and fetch data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# git clone https://github.com/NVIDIA-AI-Blueprints/data-flywheel.git\n",
    "# cd data-flywheel\n",
    "# sudo apt-get update && sudo apt-get install -y git-lfs\n",
    "# git lfs install\n",
    "# git-lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Set up paths and installs python dependencies for notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "# project_root = notebook_dir / \"data-flywheel\"\n",
    "# data_dir = project_root / \"data\"\n",
    "project_root = notebook_dir.parent\n",
    "data_dir = Path(project_root, \"data\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "os.chdir(project_root)\n",
    "print(f\"Working directory changed to: {Path.cwd()}\")\n",
    "\n",
    "# user_site = Path.home() / \".local\" / \"lib\" / f\"python{sys.version_info.major}.{sys.version_info.minor}\" / \"site-packages\"\n",
    "# if str(user_site) not in sys.path:\n",
    "#     sys.path.append(str(user_site))\n",
    "#     print(f\"Added user site-packages to sys.path: {user_site}\")\n",
    "\n",
    "# %pip install --user elasticsearch==8.17.2 pydantic-settings>=2.9.1 pandas>=2.2.3 matplotlib==3.10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Update `config/config.yaml` to use remote LLM as judge. By default, data flywheel blueprint deploys `LLama-3.3-70B-instruct` locally for LLM as a judge, which requires 4 GPUs. But for the launchable, we will choose the remote LLM judge and use the `LLama-3.3-70B-instruct` NIM hosted on [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct).\n",
    "\n",
    "By default, only `Llama-3.2-1b-instruct` will be used in the flywheel but you can uncomment other models in the yaml file to include in the flywheel run. You can also change other config settings such as data split and training hyperparameters as desired\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "config_path = project_root / \"config\" / \"config.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    original_yaml = f.read()\n",
    "\n",
    "llm_judge_config_block = \"\"\"llm_judge_config:\n",
    "  type: \"remote\"\n",
    "  url: \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "  model_id: \"meta/llama-3.3-70b-instruct\"\n",
    "  api_key_env: \"NGC_API_KEY\"\n",
    "\"\"\"\n",
    "updated_yaml = re.sub(\n",
    "    r\"llm_judge_config:.*?(?=\\n\\w|\\Z)\",  # stops at next top-level key\n",
    "    llm_judge_config_block,\n",
    "    original_yaml,\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write(updated_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Start data flywheel service, which involves first deploying the Nemo Microservices Platform and then bring up the data flywheel service via docker compose. This step take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# How to remove all running containers\n",
    "os.system(\"docker rm -f $(docker ps -q)\")\n",
    "\n",
    "# How to delete folders\n",
    "if os.path.exists(\"nemo-microservices-helm-chart-25.4.0.tgz\"):\n",
    "    os.rmdir(\"nemo-microservices-helm-chart-25.4.0.tgz\")\n",
    "\n",
    "# How to delete minikube\n",
    "os.system(\"minikube delete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "log() {\n",
    "  echo -e \"\\033[1;32m[INFO]\\033[0m $1\"\n",
    "}\n",
    "\n",
    "echo \"$NGC_API_KEY\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "chmod +x scripts/deploy-nmp.sh scripts/run.sh\n",
    "\n",
    "log \"Starting Nemo Microservices Platform (NMP) deployment...\"\n",
    "./scripts/deploy-nmp.sh >> flywheel_deploy.log 2>&1\n",
    "log \"NMP deployed successfully!\"\n",
    "\n",
    "log \"Starting data flywheel service...\"\n",
    "./scripts/run.sh >> flywheel_deploy.log 2>&1\n",
    "log \"Data flywheel service started successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"1\"></a>\n",
    "## Step 1: Load Sample Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import required libraries and configure pandas display options for better readability in notebook outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Width of the display in characters\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided sample dataset from AI Virtual Assistant (`aiva`) (`data/aiva_primary_assistant_dataset.jsonl`) to simulate real user logs captured while an agentic customer service agent application is running. Each data point has the following schema:\n",
    "\n",
    "| Field        | Type               | Description                                                         |\n",
    "|--------------|--------------------|---------------------------------------------------------------------|\n",
    "| `timestamp`  | `int` (epoch secs) | Time the request was issued                                         |\n",
    "| `workload_id`| `str`              | Stable identifier for the logical task / route / agent node         |\n",
    "| `client_id`  | `str`              | Identifier of the application or deployment that generated traffic  |\n",
    "| `request`    | `dict`             | Exact [`openai.ChatCompletion.create`](https://platform.openai.com/docs/api-reference/chat/create) payload received by the model |\n",
    "| `response`   | `dict`             | Exact `ChatCompletion` response returned by the model               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reuqest` uses the OpenAI `ChatCompletions` request format and contains the following attributes:\n",
    "\n",
    "- `model` includes the Model ID used to generate the response.\n",
    "- `messages` includes a `system` message as well as a `user` query.\n",
    "- `tools` includes a list of functions and parameters available to the LLM to choose from, as well as their parameters and descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = data_dir / \"aiva_primary_assistant_dataset.jsonl\"\n",
    "\n",
    "!head -n1 {DATA_PATH} | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data points generated by AI Virtual Assistant in response to user queries are considered **ground truth**. \n",
    "\n",
    "Ground truth data points are used to **evaluate** and **customize** more efficient models that can perform similarly to the current model. This customization process is analogous to a student-teacher distillation setup, where synthetic data generated from the teacher model is used to fine-tune a student model.\n",
    "\n",
    "Next, we'll load the data into Elasticsearch using a helper method `load_data_to_elasticsearch`, making it accessible to the Flywheel Orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.load_test_data_es import load_data_to_elasticsearch\n",
    "\n",
    "load_data_to_elasticsearch(file_path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.load_test_data_weave import load_data_to_weave\n",
    "\n",
    "# load_data_to_weave(file_path=DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2\"></a>\n",
    "## Step 2: Create a Flywheel Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a Flywheel job by sending a POST request to the `/jobs` API. This triggers the workflow asynchronously.\n",
    "\n",
    "In production environments, you can automate this process to run at scheduled intervals, in response to specific events, or on demand.\n",
    "\n",
    "For this tutorial, we will target the primary customer service agent by setting the `workload_id` to \"primary_assistant\" and we will set `client_id` to \"aiva-1\" which has 300 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flywheel Orchestrator URL\n",
    "API_BASE_URL = \"http://0.0.0.0:8000\"\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{API_BASE_URL}/api/jobs\",\n",
    "    json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-1\"}\n",
    ")\n",
    "\n",
    "response.raise_for_status()\n",
    "job_id = response.json()[\"id\"]\n",
    "\n",
    "print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3\"></a>\n",
    "## Step 3: Monitor Job Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a GET request to `/jobs/{job_id}` to retrieve the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_status(job_id):\n",
    "    \"\"\"Get the current status of a job.\"\"\"\n",
    "    response = requests.get(f\"{API_BASE_URL}/api/jobs/{job_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process, you can define utility functions that:\n",
    "\n",
    "- Periodically retrieve the job status\n",
    "- Format the output into a table\n",
    "\n",
    "This makes it easier to compare and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polling interval (in seconds) for monitoring flywheel job\n",
    "POLL_INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def format_runtime(seconds):\n",
    "    \"\"\"Format runtime in seconds to a human-readable string.\"\"\"\n",
    "    if seconds is None:\n",
    "        return \"-\"\n",
    "    minutes, seconds = divmod(seconds, 60)\n",
    "    if minutes > 0:\n",
    "        return f\"{int(minutes)}m {int(seconds)}s\"\n",
    "    return f\"{int(seconds)}s\"\n",
    "\n",
    "def extract_main_score(score_str):\n",
    "    try:\n",
    "        first_score = score_str.split(\";\")[0]\n",
    "        value = first_score.split(\":\")[1].strip()\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def create_results_table(job_data):\n",
    "    \"\"\"Create a pandas DataFrame from job data.\"\"\"\n",
    "    rows = []\n",
    "    for nim in job_data[\"nims\"]:\n",
    "        model_name = nim[\"model_name\"]\n",
    "        for eval in nim[\"evaluations\"]:\n",
    "            score_str = \"; \".join(f\"{k}: {v}\" for k, v in eval[\"scores\"].items() if k != \"function_name_and_args_accuracy\")\n",
    "            main_score = extract_main_score(score_str)\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Eval Type\": eval[\"eval_type\"].upper(),\n",
    "                \"Score\": main_score,\n",
    "                \"Percent Done\": eval[\"progress\"],\n",
    "                \"Runtime\": format_runtime(eval[\"runtime_seconds\"]),\n",
    "                \"Status\": \"Completed\" if eval[\"finished_at\"] else \"Running\",\n",
    "                \"Started\": datetime.fromisoformat(eval[\"started_at\"]).strftime(\"%H:%M:%S\"),\n",
    "                \"Finished\": datetime.fromisoformat(eval[\"finished_at\"]).strftime(\"%H:%M:%S\") if eval[\"finished_at\"] else \"-\"\n",
    "            })\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"Model\", \"Eval Type\", \"Scores\", \"Percent Done\", \"Runtime\", \"Status\", \"Started\", \"Finished\"])\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    return df.sort_values([\"Model\", \"Eval Type\"])\n",
    "\n",
    "def create_customization_table(job_data):\n",
    "    \"\"\"Create a pandas DataFrame from customization data.\"\"\"\n",
    "    customizations = []\n",
    "    for nim in job_data[\"nims\"]:\n",
    "        model_name = nim[\"model_name\"]\n",
    "        for custom in nim[\"customizations\"]:\n",
    "            customizations.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Started\": datetime.fromisoformat(custom[\"started_at\"]).strftime(\"%H:%M:%S\"),\n",
    "                \"Epochs Completed\": custom[\"epochs_completed\"],\n",
    "                \"Steps Completed\": custom[\"steps_completed\"],\n",
    "                \"Finished\": datetime.fromisoformat(custom[\"finished_at\"]).strftime(\"%H:%M:%S\") if custom[\"finished_at\"] else \"-\",\n",
    "                \"Status\": \"Completed\" if custom[\"finished_at\"] else \"Running\",\n",
    "                \"Runtime\": format_runtime(custom[\"runtime_seconds\"]),\n",
    "                \"Percent Done\": custom[\"progress\"],\n",
    "            })\n",
    "   \n",
    "    if not customizations:\n",
    "        customizations = pd.DataFrame(columns=[\"Model\", \"Started\", \"Epochs Completed\", \"Steps Completed\", \"Finished\", \"Runtime\", \"Percent Done\"])\n",
    "    customizations = pd.DataFrame(customizations)\n",
    "    return customizations.sort_values([\"Model\"])\n",
    "\n",
    "def monitor_job(job_id):\n",
    "    \"\"\"Monitor a job and display its progress in a table.\"\"\"\n",
    "    print(f\"Monitoring job {job_id}...\")\n",
    "    print(\"Press Ctrl+C to stop monitoring\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            job_data = get_job_status(job_id)\n",
    "            results_df = create_results_table(job_data)\n",
    "            customizations_df = create_customization_table(job_data)\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Job Status: {job_data['status']}\")\n",
    "            print(f\"Total Records: {job_data['num_records']}\")\n",
    "            print(f\"Last Updated: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            print(\"\\nResults:\")\n",
    "            display(results_df)\n",
    "            print(\"\\nCustomizations:\")\n",
    "            display(customizations_df)\n",
    "            display(job_data)\n",
    "\n",
    "            # Plot 1: Evaluation Scores\n",
    "            ax.set_title(\"Evalulation Results\", fontsize=14)\n",
    "            if not results_df.empty:\n",
    "                pivot_df = results_df.pivot(index=\"Model\", columns=\"Eval Type\", values=\"Score\").fillna(0)\n",
    "                pivot_df.plot(kind='bar', ax=ax)\n",
    "                ax.set_ylabel(\"Eval Metrics\")\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.legend(title=\"Eval Type\")\n",
    "                ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"No Evaluation Data\", ha='center', va='center')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()                        \n",
    "            time.sleep(POLL_INTERVAL)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nMonitoring stopped by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring the job\n",
    "monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youâ€™ve now successfully completed a Flywheel run and can review the evaluation results to decide whether to promote the model. However, with only 300 data points, the customized `Llama-3.2-1B-instruct` is likely still limited in performance.\n",
    "\n",
    "That said, the Data Flywheel operates as a self-reinforcing cycleâ€”models continue to improve as more user interaction logs are collected. Below, we demonstrate how model performance improves incrementally with additional data.\n",
    "\n",
    "\n",
    "![aiva1](./img/300dp.png)\n",
    "\n",
    "**Flywheel run results at 300 data points**\n",
    "\n",
    "![aiva2](./img/500dp.png)\n",
    "\n",
    "**Flywheel run results at 500 data points**\n",
    "\n",
    "![aiva3](./img/1000dp.png)\n",
    "\n",
    "**Flywheel run results at 1,000 data points**\n",
    "\n",
    "With the improvement results demonstrated, you can now move on to Step 4 to run the Flywheel with additional data yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Show Continuous Improvement (Optional)\n",
    "\n",
    "To extend the flywheel run with additional data, weâ€™ll launch a new job using `client_id` set to \"aiva-2\", which includes **500** data points, to evaluate the impact of increased data volume on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.post(\n",
    "#     f\"{API_BASE_URL}/api/jobs\",\n",
    "#     json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-2\"}\n",
    "# )\n",
    "\n",
    "# response.raise_for_status()\n",
    "# job_id = response.json()[\"id\"]\n",
    "\n",
    "# print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_job_status(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see some improvements of the customized model compared to the last run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have now collected even more data points, let's kick off another flywheel run by setting `client_id` to \"aiva-3\" which includes **1,000** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.post(\n",
    "#     f\"{API_BASE_URL}/api/jobs\",\n",
    "#     json={\"workload_id\": \"primary_assistant\", \"client_id\": \"aiva-3\"}\n",
    "# )\n",
    "\n",
    "# response.raise_for_status()\n",
    "# job_id = response.json()[\"id\"]\n",
    "\n",
    "# print(f\"Created job with ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run with 1,000 data points, we should observe the customized modelâ€™s score approaching 1.0. This indicates that the `LLama-3.2-1B-instruct` model achieves accuracy comparable to the much larger `LLama-3.3-70B-instruct` base model deployed in AI Virtual Assistant, while significantly reducing latency and compute usage thanks to its smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
